/*
Copyright 2022 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package integration

import (
	"context"
	"flag"
	"fmt"
	"os"
	"testing"
	"time"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/uuid"
	"k8s.io/client-go/kubernetes"
	clientset "k8s.io/client-go/kubernetes"
	"k8s.io/klog/v2"
	"k8s.io/kubernetes/pkg/scheduler"
	schedapi "k8s.io/kubernetes/pkg/scheduler/apis/config"
	fwkruntime "k8s.io/kubernetes/pkg/scheduler/framework/runtime"
	st "k8s.io/kubernetes/pkg/scheduler/testing"

	topologyv1alpha1 "github.com/k8stopologyawareschedwg/noderesourcetopology-api/pkg/apis/topology/v1alpha1"
	"github.com/k8stopologyawareschedwg/noderesourcetopology-api/pkg/generated/clientset/versioned"

	schedconfig "sigs.k8s.io/scheduler-plugins/apis/config"
	"sigs.k8s.io/scheduler-plugins/pkg/noderesourcetopology"
	"sigs.k8s.io/scheduler-plugins/test/util"
)

const (
	defaultCacheResyncPeriodSeconds = 5
	anyNode                         = "*"
)

type podDesc struct {
	podName      string
	isGuaranteed bool
	isDelete     bool
	resourcesMap map[string]string
	expectedNode string
	// autogenerated
	pod *corev1.Pod
}

func (p *podDesc) SetupPod(ns string, initContainer bool) {
	pt := st.MakePod().Namespace(ns).Name(p.podName)
	if p.isGuaranteed {
		p.pod = util.WithLimits(pt, p.resourcesMap, initContainer).Obj()
	} else {
		p.pod = util.WithRequests(pt, p.resourcesMap, initContainer).Obj()
	}
}

type testCase struct {
	name                   string
	nodeResourceTopologies []*topologyv1alpha1.NodeResourceTopology
	podDescs               []podDesc
}

func init() {
	klog.InitFlags(nil)
}

func TestTopologyCachePluginWithoutUpdates(t *testing.T) {
	verbose := "0"
	if val, ok := os.LookupEnv("SCHED_PLUGINS_TEST_VERBOSE"); ok {
		verbose = val
	}

	cacheResyncPeriod := int64(defaultCacheResyncPeriodSeconds)

	os.Args = []string{"unused", "-logtostderr", "-v", verbose}
	klog.Infof("args = %v", os.Args[1:])
	flag.Parse()

	// key: BE: Best Effort QoS; BU: BUrstable QoS; GU: GUaranteed QoS
	for _, tt := range []testCase{
		// BE pods: not impacted at all (no resource tracking)
		{
			name: "BU pod: pessimistic cache overallocation not impactful, pod to be scheduled",
			podDescs: []podDesc{
				{
					podName:      "nrt-bu-pod-1000",
					isGuaranteed: false,
					resourcesMap: map[string]string{
						string(corev1.ResourceCPU):    "16",
						string(corev1.ResourceMemory): "24Gi",
					},
					expectedNode: anyNode,
				},
				{
					podName:      "nrt-bu-pod-2000",
					isGuaranteed: false,
					resourcesMap: map[string]string{
						string(corev1.ResourceCPU):    "16",
						string(corev1.ResourceMemory): "24Gi",
					},
					expectedNode: anyNode,
				},
			},
			nodeResourceTopologies: []*topologyv1alpha1.NodeResourceTopology{
				MakeNRT().Name("fake-node-cache-1").Policy(topologyv1alpha1.SingleNUMANodeContainerLevel).
					Zone(
						topologyv1alpha1.ResourceInfoList{
							noderesourcetopology.MakeTopologyResInfo(cpu, "32", "30"),
							noderesourcetopology.MakeTopologyResInfo(memory, "64Gi", "60Gi"),
						}).
					Zone(
						topologyv1alpha1.ResourceInfoList{
							noderesourcetopology.MakeTopologyResInfo(cpu, "32", "30"),
							noderesourcetopology.MakeTopologyResInfo(memory, "64Gi", "62Gi"),
						}).Obj(),
				MakeNRT().Name("fake-node-cache-2").Policy(topologyv1alpha1.SingleNUMANodeContainerLevel).
					Zone(
						topologyv1alpha1.ResourceInfoList{
							noderesourcetopology.MakeTopologyResInfo(cpu, "32", "10"),
							noderesourcetopology.MakeTopologyResInfo(memory, "64Gi", "14Gi"),
						}).
					Zone(
						topologyv1alpha1.ResourceInfoList{
							noderesourcetopology.MakeTopologyResInfo(cpu, "32", "8"),
							noderesourcetopology.MakeTopologyResInfo(memory, "64Gi", "10Gi"),
						}).Obj(),
			},
		},
		{
			name: "BE pod: with devices, pessimistic cache overallocation prevents pod to be scheduled",
			podDescs: []podDesc{
				{
					podName:      "nrt-be-pod-3000",
					isGuaranteed: true, // aka set resourcesMap in limits
					resourcesMap: map[string]string{
						nicResourceName: "2",
					},
					expectedNode: "fake-node-cache-2",
				},
				{
					podName:      "nrt-be-pod-4000",
					isGuaranteed: true, // aka set resourcesMap in limits
					resourcesMap: map[string]string{
						nicResourceName: "2",
					},
					expectedNode: "",
				},
			},
			nodeResourceTopologies: []*topologyv1alpha1.NodeResourceTopology{
				MakeNRT().Name("fake-node-cache-1").Policy(topologyv1alpha1.SingleNUMANodeContainerLevel).
					Zone(
						topologyv1alpha1.ResourceInfoList{
							noderesourcetopology.MakeTopologyResInfo(cpu, "32", "30"),
							noderesourcetopology.MakeTopologyResInfo(memory, "64Gi", "62Gi"),
						}).
					Zone(
						topologyv1alpha1.ResourceInfoList{
							noderesourcetopology.MakeTopologyResInfo(cpu, "32", "30"),
							noderesourcetopology.MakeTopologyResInfo(memory, "64Gi", "62Gi"),
						}).Obj(),
				MakeNRT().Name("fake-node-cache-2").Policy(topologyv1alpha1.SingleNUMANodeContainerLevel).
					Zone(
						topologyv1alpha1.ResourceInfoList{
							noderesourcetopology.MakeTopologyResInfo(cpu, "32", "30"),
							noderesourcetopology.MakeTopologyResInfo(memory, "64Gi", "62Gi"),
							noderesourcetopology.MakeTopologyResInfo(nicResourceName, "2", "2"),
						}).
					Zone(
						topologyv1alpha1.ResourceInfoList{
							noderesourcetopology.MakeTopologyResInfo(cpu, "32", "30"),
							noderesourcetopology.MakeTopologyResInfo(memory, "64Gi", "62Gi"),
							noderesourcetopology.MakeTopologyResInfo(nicResourceName, "2", "2"),
						}).Obj(),
			},
		},
		{
			name: "GU pod: pessimistic cache overallocation prevents pod to be scheduled",
			podDescs: []podDesc{
				{
					podName:      "nrt-pod-1000",
					isGuaranteed: true,
					resourcesMap: map[string]string{
						string(corev1.ResourceCPU):    "16",
						string(corev1.ResourceMemory): "24Gi",
					},
					expectedNode: "fake-node-cache-1",
				},
				{
					podName:      "nrt-pod-2000",
					isGuaranteed: true,
					resourcesMap: map[string]string{
						string(corev1.ResourceCPU):    "16",
						string(corev1.ResourceMemory): "24Gi",
					},
					expectedNode: "",
				},
			},
			nodeResourceTopologies: []*topologyv1alpha1.NodeResourceTopology{
				MakeNRT().Name("fake-node-cache-1").Policy(topologyv1alpha1.SingleNUMANodeContainerLevel).
					Zone(
						topologyv1alpha1.ResourceInfoList{
							noderesourcetopology.MakeTopologyResInfo(cpu, "32", "30"),
							noderesourcetopology.MakeTopologyResInfo(memory, "64Gi", "60Gi"),
						}).
					Zone(
						topologyv1alpha1.ResourceInfoList{
							noderesourcetopology.MakeTopologyResInfo(cpu, "32", "30"),
							noderesourcetopology.MakeTopologyResInfo(memory, "64Gi", "62Gi"),
						}).Obj(),
				MakeNRT().Name("fake-node-cache-2").Policy(topologyv1alpha1.SingleNUMANodeContainerLevel).
					Zone(
						topologyv1alpha1.ResourceInfoList{
							noderesourcetopology.MakeTopologyResInfo(cpu, "32", "10"),
							noderesourcetopology.MakeTopologyResInfo(memory, "64Gi", "14Gi"),
						}).
					Zone(
						topologyv1alpha1.ResourceInfoList{
							noderesourcetopology.MakeTopologyResInfo(cpu, "32", "8"),
							noderesourcetopology.MakeTopologyResInfo(memory, "64Gi", "10Gi"),
						}).Obj(),
			},
		},
		{
			name: "GU pod: pessimistic cache overallocation ignores deletes prevents pod to be scheduled",
			podDescs: []podDesc{
				{
					podName:      "nrt-pod-3000",
					isGuaranteed: true,
					resourcesMap: map[string]string{
						string(corev1.ResourceCPU):    "16",
						string(corev1.ResourceMemory): "24Gi",
					},
					expectedNode: "fake-node-cache-1",
				},
				{
					podName:  "nrt-pod-3000",
					isDelete: true,
				},
				{
					podName:      "nrt-pod-4000",
					isGuaranteed: true,
					resourcesMap: map[string]string{
						string(corev1.ResourceCPU):    "16",
						string(corev1.ResourceMemory): "24Gi",
					},
					expectedNode: "",
				},
			},
			nodeResourceTopologies: []*topologyv1alpha1.NodeResourceTopology{
				MakeNRT().Name("fake-node-cache-1").Policy(topologyv1alpha1.SingleNUMANodeContainerLevel).
					Zone(
						topologyv1alpha1.ResourceInfoList{
							noderesourcetopology.MakeTopologyResInfo(cpu, "32", "30"),
							noderesourcetopology.MakeTopologyResInfo(memory, "64Gi", "60Gi"),
						}).
					Zone(
						topologyv1alpha1.ResourceInfoList{
							noderesourcetopology.MakeTopologyResInfo(cpu, "32", "30"),
							noderesourcetopology.MakeTopologyResInfo(memory, "64Gi", "62Gi"),
						}).Obj(),
				MakeNRT().Name("fake-node-cache-2").Policy(topologyv1alpha1.SingleNUMANodeContainerLevel).
					Zone(
						topologyv1alpha1.ResourceInfoList{
							noderesourcetopology.MakeTopologyResInfo(cpu, "32", "10"),
							noderesourcetopology.MakeTopologyResInfo(memory, "64Gi", "14Gi"),
						}).
					Zone(
						topologyv1alpha1.ResourceInfoList{
							noderesourcetopology.MakeTopologyResInfo(cpu, "32", "8"),
							noderesourcetopology.MakeTopologyResInfo(memory, "64Gi", "10Gi"),
						}).Obj(),
			},
		},
	} {
		t.Run(tt.name, func(t *testing.T) {
			// because caching, each testcase needs to run from a clean slate
			testCtx := &testContext{}
			testCtx.Ctx, testCtx.CancelFn = context.WithCancel(context.Background())

			cs := kubernetes.NewForConfigOrDie(globalKubeConfig)
			extClient := versioned.NewForConfigOrDie(globalKubeConfig)
			testCtx.ClientSet = cs
			testCtx.KubeConfig = globalKubeConfig

			if err := waitForNRT(cs); err != nil {
				t.Fatalf("Timed out waiting for CRD to be ready: %v", err)
			}

			ns := fmt.Sprintf("integration-test-%v", string(uuid.NewUUID()))
			createNamespace(t, testCtx, ns)

			cfg, err := util.NewDefaultSchedulerComponentConfig()
			if err != nil {
				t.Fatal(err)
			}
			cfg.Profiles[0].Plugins.Filter.Enabled = append(cfg.Profiles[0].Plugins.Filter.Enabled, schedapi.Plugin{Name: noderesourcetopology.Name})
			cfg.Profiles[0].Plugins.Reserve.Enabled = append(cfg.Profiles[0].Plugins.Reserve.Enabled, schedapi.Plugin{Name: noderesourcetopology.Name})
			cfg.Profiles[0].Plugins.Score.Enabled = append(cfg.Profiles[0].Plugins.Score.Enabled, schedapi.Plugin{Name: noderesourcetopology.Name})
			cfg.Profiles[0].PluginConfig = append(cfg.Profiles[0].PluginConfig, schedapi.PluginConfig{
				Name: noderesourcetopology.Name,
				Args: &schedconfig.NodeResourceTopologyMatchArgs{
					ScoringStrategy:          schedconfig.ScoringStrategy{Type: schedconfig.LeastAllocated},
					CacheResyncPeriodSeconds: cacheResyncPeriod,
				},
			})

			defer func() {
				cleanupTest(t, testCtx)
				klog.Infof("test environment cleaned up")
			}()

			if err := createNodesFromNodeResourceTopologies(cs, testCtx.Ctx, tt.nodeResourceTopologies); err != nil {
				t.Fatalf("%v", err)
			}

			var pods []*corev1.Pod
			for idx := range tt.podDescs {
				p := &tt.podDescs[idx]
				if p.isDelete {
					continue
				}

				p.SetupPod(ns, false)
				pods = append(pods, p.pod)
				klog.Infof("Prepared pod: %s", p.pod.Name)
			}

			t.Logf("Start-topology-match-cache-test %q", tt.name)
			defer cleanupNodeResourceTopologies(testCtx.Ctx, extClient, tt.nodeResourceTopologies)
			defer func() {
				cleanupPods(t, testCtx, pods)
				klog.Infof("Pods cleaned up")
			}()

			klog.Infof("Creating %d NRT objects", len(tt.nodeResourceTopologies))
			if err := createNodeResourceTopologies(testCtx.Ctx, extClient, tt.nodeResourceTopologies); err != nil {
				t.Fatal(err)
			}

			testCtx = initTestSchedulerWithOptions(
				t,
				testCtx,
				scheduler.WithProfiles(cfg.Profiles...),
				scheduler.WithFrameworkOutOfTreeRegistry(fwkruntime.Registry{noderesourcetopology.Name: noderesourcetopology.New}),
			)
			syncInformerFactory(testCtx)
			go testCtx.Scheduler.Run(testCtx.Ctx)
			klog.Infof("init scheduler success")

			for idx := range tt.podDescs {
				p := &tt.podDescs[idx]
				if p.isDelete {
					var err error
					klog.Infof("Waiting before to delete Pod %q", p.podName)
					err = podIsScheduled(1*time.Second, 20, cs, ns, p.podName)
					if err != nil {
						t.Errorf("Pod %q to be scheduled, error: %v", p.pod.Name, err)
					}

					klog.Infof("Deleting Pod %q", p.podName)
					err = cs.CoreV1().Pods(ns).Delete(testCtx.Ctx, p.podName, metav1.DeleteOptions{})
					if err != nil {
						t.Fatalf("Failed to delete Pod %q: %v", p.podName, err)
					}
				} else {
					klog.Infof("Creating Pod %q", p.pod.Name)
					_, err := cs.CoreV1().Pods(ns).Create(testCtx.Ctx, p.pod, metav1.CreateOptions{})
					if err != nil {
						t.Fatalf("Failed to create Pod %q: %v", p.pod.Name, err)
					}
				}
			}

			for _, p := range tt.podDescs {
				if p.isDelete {
					continue
				}

				var action string = "scheduled"
				var checkPod func(interval time.Duration, times int, cs clientset.Interface, podNamespace, podName string) error = podIsScheduled
				if p.expectedNode == "" {
					action = "kept pending"
					checkPod = podIsPending
				}

				err = checkPod(1*time.Second, 20, cs, p.pod.Namespace, p.pod.Name)
				if err != nil {
					t.Errorf("Pod %q to be %s, error: %v", p.pod.Name, action, err)
				}
				klog.Infof("Pod %v %s", p.pod.Name, action)
			}

			for _, p := range tt.podDescs {
				if p.isDelete {
					continue
				}

				nodeName, err := getNodeName(testCtx.Ctx, cs, ns, p.pod.Name)
				klog.Infof("Pod %s scheduled on node %q (expected %q)", p.pod.Name, nodeName, p.expectedNode)
				if err != nil {
					klog.Infof("%v", err)
				}
				if p.expectedNode == nodeName {
					t.Logf("Pod %q is on a nodes as expected.", p.pod.Name)
				} else if p.expectedNode == anyNode {
					if nodeName == "" {
						t.Errorf("Pod %q is expected to be running on any node, but still pending", p.pod.Name)
					} else {
						t.Logf("Pod %q is running, any node is fine (currently on %q)", p.pod.Name, nodeName)
					}
				} else if p.expectedNode == "" {
					t.Errorf("Pod %q is expected to be pending, but found on node %q", p.pod.Name, nodeName)
				} else {
					t.Errorf("Pod %q is expected on node %q, but found on node %q", p.pod.Name, p.expectedNode, nodeName)
				}
			}

			t.Logf("Case %v finished", tt.name)
		})
	}
}
